{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial recognition using random forests\n",
    "Carl Ehrett, 2019-04-30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block are all the variables that will (potentially) need to be redefined by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10 # Number of samples to record (of each person)\n",
    "everyFrame = 24 # record an observation once every everyFrame frames\n",
    "shapePath = ('C:\\\\Users\\\\carle\\\\Documents\\\\Python\\\\' # This is the path to the facial landmarks data file\n",
    "              'Math 9810 Machine Learning\\\\'\n",
    "              'shape_predictor_68_face_landmarks.dat')\n",
    "camSrc = 0 # This should set the webcam to be the input, but you may need to change the value on your system\n",
    "personNames = [['CARL'],['ERIN']] # Names of the people being recognized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all needed libraries. Note that you need to have all these libraries installed in order to use this Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import cv2 # This one is the OpenCV library.\n",
    "import matplotlib.pyplot as plt ; plt.rcdefaults\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the functions which will be used, in conjunction with the facial landmarks, to define covariates. The `getArea()` function takes as input three points (in 2D space) and returns the area of the triangle those points define. Similarly, `getAngle()` returns the angle formed by its three input points. Finally, `getLength()` takes two points as input, and returns the distance between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArea(x,y,z):\n",
    "    a = x-y\n",
    "    b = z-y\n",
    "    area = np.abs( a[0] * b[1] - a[1] * b[0] ) / 2\n",
    "    return area\n",
    "\n",
    "def getAngle(x,y,z):\n",
    "    a = x-y\n",
    "    b = z-y\n",
    "    cosineAngle = np.dot(a,b) / ( np.linalg.norm(a) * np.linalg.norm(b) )\n",
    "    angle = np.arccos(cosineAngle)\n",
    "    return angle\n",
    "    \n",
    "def getLength(x,y):\n",
    "    length = np.linalg.norm(x-y)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to gather facial covariates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def facedat(scaledShape):\n",
    "    dat = [\n",
    "        ### Lengths    \n",
    "        # Outer eyes width:\n",
    "        getLength(scaledShape[45,],scaledShape[36,]),\n",
    "        # Inner eyes width:\n",
    "        getLength(scaledShape[42,],scaledShape[39,]),\n",
    "        # Nose length:\n",
    "        getLength(scaledShape[27,],scaledShape[33,]),\n",
    "        # Nose width:\n",
    "        getLength(scaledShape[35,],scaledShape[31,]),\n",
    "        # Outer mouth width:\n",
    "        getLength(scaledShape[54,],scaledShape[48,]),\n",
    "        # Inner mouth width:\n",
    "        getLength(scaledShape[64,],scaledShape[60,]),\n",
    "        # Outer mouth height:\n",
    "        getLength(scaledShape[51,],scaledShape[57,]),\n",
    "        # Inner mouth height:\n",
    "        getLength(scaledShape[62,],scaledShape[66,]),\n",
    "        # Jaw to eye, left and right:\n",
    "        getLength(scaledShape[36,],scaledShape[10,]) +\n",
    "        getLength(scaledShape[16,],scaledShape[45,]),\n",
    "        # Lower jaw width:\n",
    "        getLength(scaledShape[12,],scaledShape[4,]),\n",
    "        # Eye to mouth, left and right:\n",
    "        getLength(scaledShape[39,],scaledShape[48,]) +\n",
    "        getLength(scaledShape[42,],scaledShape[54,]),\n",
    "        # Eyebrow widths, left and right:\n",
    "        getLength(scaledShape[21,],scaledShape[17,]) + \n",
    "        getLength(scaledShape[22,],scaledShape[26,]),\n",
    "        # Nose to mouth:\n",
    "        getLength(scaledShape[33,],scaledShape[51,]),\n",
    "        # Outer eye to eyebrow, left and right:\n",
    "        getLength(scaledShape[17,],scaledShape[36,]) +\n",
    "        getLength(scaledShape[26,],scaledShape[45,]),\n",
    "        # Inner eye to eyebrow, left and right:\n",
    "        getLength(scaledShape[21,],scaledShape[39,]) +\n",
    "        getLength(scaledShape[22,],scaledShape[42,]),\n",
    "        # Mouth to lower jaw, left and right:\n",
    "        getLength(scaledShape[4,],scaledShape[48,]) +\n",
    "        getLength(scaledShape[12,],scaledShape[54,]),\n",
    "        # Mouth to chin:\n",
    "        getLength(scaledShape[57,],scaledShape[8,]),\n",
    "        # Inner eyebrow width:\n",
    "        getLength(scaledShape[21,],scaledShape[22,]),\n",
    "        # Total jaw length\n",
    "        np.sum(np.linalg.norm(\n",
    "                np.diff(scaledShape[:17,],axis=0),axis=1)),\n",
    "        #\n",
    "        ### Areas\n",
    "        # Nose area:\n",
    "        getArea(scaledShape[27,],scaledShape[31,],scaledShape[33,]),\n",
    "        # Eye-nose area, left and right:\n",
    "        getArea(scaledShape[39,],scaledShape[27,],scaledShape[31,]) +\n",
    "        getArea(scaledShape[42,],scaledShape[27,],scaledShape[35,]),\n",
    "        # Eye-mouth area, left and right:\n",
    "        getArea(scaledShape[36,],scaledShape[39,],scaledShape[48,]) +\n",
    "        getArea(scaledShape[42,],scaledShape[45,],scaledShape[54,]),\n",
    "        # Nose-mouth area, left and right:\n",
    "        getArea(scaledShape[31,],scaledShape[33,],scaledShape[51,]) +\n",
    "        getArea(scaledShape[33,],scaledShape[51,],scaledShape[35,]),\n",
    "        # Eyebrow-outer eye area, left and right:\n",
    "        getArea(scaledShape[17,],scaledShape[21,],scaledShape[36,]) + \n",
    "        getArea(scaledShape[22,],scaledShape[26,],scaledShape[45,]),\n",
    "        # Eye-eyebrow-jaw area, left and right:\n",
    "        getArea(scaledShape[0,],scaledShape[17,],scaledShape[36,]) +\n",
    "        getArea(scaledShape[16,],scaledShape[26,],scaledShape[45,]),\n",
    "        # Eye-mouth-jaw area, left and right:\n",
    "        getArea(scaledShape[0,],scaledShape[36,],scaledShape[48,]) +\n",
    "        getArea(scaledShape[16,],scaledShape[45,],scaledShape[54,]),\n",
    "        # Mouth-lower jaw-chin area, left and right:\n",
    "        getArea(scaledShape[48,],scaledShape[4,],scaledShape[8,]) +\n",
    "        getArea(scaledShape[54,],scaledShape[12,],scaledShape[8,]),\n",
    "        # \n",
    "        ### Angles\n",
    "        # Eye-nose-mouth angles, left and right\n",
    "        getAngle(scaledShape[31,],scaledShape[39,],scaledShape[48,]) + \n",
    "        getAngle(scaledShape[35,],scaledShape[42,],scaledShape[54,]),\n",
    "        # Mouth-jaw-lower jaw angles, left and right\n",
    "        getAngle(scaledShape[48,],scaledShape[0,],scaledShape[4,]) + \n",
    "        getAngle(scaledShape[54,],scaledShape[16,],scaledShape[12,]),\n",
    "        # Nose-eye-eyebrow angles, left and right\n",
    "        getAngle(scaledShape[27,],scaledShape[39,],scaledShape[21,]) +\n",
    "        getAngle(scaledShape[27,],scaledShape[42,],scaledShape[22,])\n",
    "    ]\n",
    "    return(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function that takes a set of facial landmarks, and rotates them (so face is vertical) and scales them with respect to height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleFace(shape):\n",
    "    chinPt = shape[8]\n",
    "    scaledShape = shape - chinPt\n",
    "    # Get and correct face angle\n",
    "    faceAngle = np.arctan(\n",
    "            (scaledShape[16,1]-scaledShape[0,1])/(\n",
    "                    scaledShape[16,0]-scaledShape[0,0]))\n",
    "    cosFA = np.cos(faceAngle)\n",
    "    sinFA = np.sin(faceAngle)\n",
    "    rotMat = [[cosFA, -sinFA],[sinFA, cosFA]]\n",
    "    scaledShape = np.matmul(scaledShape,rotMat)\n",
    "    faceWidth = scaledShape[16,0] - scaledShape[0,0]\n",
    "    faceHeight = scaledShape[8,1] - scaledShape[27,1]\n",
    "    scaledShape = scaledShape / [faceWidth, faceHeight]\n",
    "    return(scaledShape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some useful variables, allocate some space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 30 # Number of covariates to record\n",
    "X = np.zeros([2*n,p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the dlib facial detector, and use the facial landmark data to make the facial landmark predictor. The predictor (`predictor`) is what will give us the 2D locations of the landmarks of faces that appear in the webcam stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(shapePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the video stream thread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = VideoStream(src=camSrc).start()\n",
    "time.sleep(1.0) # Just to make sure the camera has time to wake up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a loop that continually reads and outputs the webcam input, and (every `everyFrame` frames) collects a sample of the face that appears in the video feed. For data collection, only one person should present their face to the webcam. After `n` samples are collected, the script will prompt the user to show a different face to the webcam for the second half of the data collection process.\n",
    "\n",
    "If your webcam feed isn't updating, that is possibly because it's not detecting any faces; the webcam only outputs when a face is found. Make sure you have good lighting on your face. Also, the facial landmark detector does pretty poorly with eyeglasses, in my experience, so you may want to remove those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii=0 # Counts how many times we've looped\n",
    "samps=0 # Counts how many samples have been gathered\n",
    "while True:\n",
    "    # grab the frame from the threaded video file stream, resize\n",
    "    # it, and convert it to grayscale\n",
    "    # channels\n",
    "    rects = []\n",
    "    while np.size(rects) == 0:\n",
    "        frame = vs.read()\n",
    "        frame = imutils.resize(frame, width=450)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        #\n",
    "        # detect faces in the grayscale frame\n",
    "        rects = detector(gray, 0)\n",
    "    \n",
    "    rect = rects[0]\n",
    "    \n",
    "    # loop over the face detections\n",
    "    if np.mod(ii,everyFrame)==0:\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "        # array\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        \n",
    "        ##############\n",
    "        # get centered version scaled wrt face width and height\n",
    "        scaledShape = scaleFace(shape)\n",
    "        \n",
    "        if n>0:\n",
    "            # Gather covariates\n",
    "            X[samps,] = facedat(scaledShape)\n",
    "        \n",
    "        samps += 1 # Increment sample counter\n",
    "        \n",
    "        ##################\n",
    "        \n",
    "    #########\n",
    "    # Draw on the face in a way that helps visualize the covariates being collected.\n",
    "    cv2.drawContours(frame, [\n",
    "            cv2.convexHull(shape[[27,33,31]]),\n",
    "            cv2.convexHull(shape[[27,33,35]]),\n",
    "            cv2.convexHull(shape[[51,33,31]]),\n",
    "            cv2.convexHull(shape[[51,33,35]]),\n",
    "            cv2.convexHull(shape[[27,39,31]]),\n",
    "            cv2.convexHull(shape[[27,35,42]]),\n",
    "            cv2.convexHull(shape[[22,45,42]]),\n",
    "            cv2.convexHull(shape[[36,39,21]]),\n",
    "            cv2.convexHull(shape[[21,27,22]]),\n",
    "            cv2.convexHull(shape[[36,39,48]]),\n",
    "            cv2.convexHull(shape[[42,45,54]]),\n",
    "            cv2.convexHull(shape[[48,54,8]]),\n",
    "            cv2.convexHull(shape[[4,48,8]]),\n",
    "            cv2.convexHull(shape[[8,54,12]]),\n",
    "            cv2.convexHull(shape[[4,48,0]]),\n",
    "            cv2.convexHull(shape[[54,12,16]]),\n",
    "            cv2.convexHull(shape[[0,36,17]]),\n",
    "            cv2.convexHull(shape[[45,26,16]]),\n",
    "            cv2.convexHull(shape[[17,21]]),\n",
    "            cv2.convexHull(shape[[22,26]]),\n",
    "            cv2.convexHull(shape[[57,8]]),\n",
    "            cv2.convexHull(shape[[51,57]]),\n",
    "            cv2.convexHull(shape[[62,66]]),\n",
    "            cv2.convexHull(shape[[60,64]])\n",
    "            ], -1, (0,255,0),1)\n",
    "        \n",
    "    \n",
    " \n",
    "    # show the frame along with how many samples have been recorded\n",
    "    cv2.putText(frame, \"SAMPLES: {}\".format(samps), (275, 30),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    #cv2.imwrite('images\\\\'+str(ii).zfill(5)+'.png',frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    " \n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\") or samps == 2*n:\n",
    "        break  \n",
    "    \n",
    "    # if we've got n samples of the first person, prompt user to switch people\n",
    "    if samps == n:\n",
    "        input(\"Switch faces and press enter to continue...\")\n",
    "        ii = -1 # Start over\n",
    "    \n",
    "    # increment counter\n",
    "    ii+=1\n",
    "    \n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the response vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.zeros([2*n,1])\n",
    "Y[n:2*n] = 1\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make and fit the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,oob_score=True)\n",
    "# Train the model\n",
    "clf.fit(X,Y.ravel())\n",
    "# Don't need data any more\n",
    "del X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the OOB score, which estimates the generalization accuracy (without the need of a test set or cross-validation). Note that this is the OOB score -- it estimates how often the classifier gets it *right*, not how often it gets it *wrong*. For the OOB error, take `1-clf.oob_score_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a bar plot of feature importances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = ('Outer eyes width','Inner eyes width','Nose length','Nose width',\n",
    "           'Outer mouth width','Inner mouth width','Outer mouth height',\n",
    "           'Inner mouth height','Jaw to eye','Lower jaw width',\n",
    "           'Eye to mouth','Eyebrow widths','Nose to mouth',\n",
    "           'Outer eye to brow','Inner eye to brow','Mouth to lower jaw',\n",
    "           'Mouth to chin','Inner brow width','Total jaw length',\n",
    "           'Nose area','Eye-nose area','Eye-mouth area','Nose-mouth area',\n",
    "           'Brow-outer eye area','Eye-brow-jaw area','Eye-mouth-jaw area',\n",
    "           'Mouth-low jaw-chin area','Eye-nose-mouth angle',\n",
    "           'Mouth-jaw-low jaw angle','Nose-eye-brow angle')\n",
    "ypos = np.arange(len(objects))\n",
    "plt.figure(figsize=(4,6))\n",
    "idxs = np.argsort(clf.feature_importances_) # We sort to list the features in order of descending importance\n",
    "plt.barh(ypos,clf.feature_importances_[idxs],align='center',alpha=0.5)\n",
    "plt.yticks(ypos,[objects[j] for j in idxs])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the facial recognition classifier. This is a loop similar to the one used to gather data, but instead of gathering training data, we are now applying the classifier to each frame of the webcam input, and annotating the image appropriately. Press `q` to exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, start the videostream again:\n",
    "vs = VideoStream(src=camSrc).start()\n",
    "time.sleep(1.0)\n",
    "\n",
    "ii=0 # Keep track of how many times we've looped\n",
    "while True:\n",
    "    # grab the frame from the threaded video file stream, resize\n",
    "    # it, and convert it to grayscale\n",
    "    # channels)\n",
    "\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width=450)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #\n",
    "    # detect faces in the grayscale frame\n",
    "    rects = detector(gray, 0)\n",
    "    \n",
    "    # loop over the face detections\n",
    "    for rect in rects:\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "        # array\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    " \n",
    "        \n",
    "        ##############\n",
    "        # get centered version scaled wrt face width and height\n",
    "        scaledShape = scaleFace(shape)\n",
    "        chinPt = shape[8] # We'll use this to place text\n",
    "        faceWidth = shape[16,0] - shape[0,0] # And this too\n",
    "        \n",
    "        # Gather covariates\n",
    "        faceInput = facedat(scaledShape)\n",
    "        \n",
    "        # Figure out to whom the face belongs\n",
    "        ident = int(clf.predict(np.array(faceInput).reshape(1,-1)))\n",
    "        txtcol = [(0,0,255),(0,255,0)][ident]\n",
    "        ident = personNames[ident]\n",
    "        \n",
    "        \n",
    "        # Annotate the image with the identity\n",
    "        cv2.putText(frame, ident[0], \n",
    "            (int(chinPt[0]-faceWidth/4),chinPt[1]+20),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.7, txtcol, 2)\n",
    "        ##################\n",
    "        \n",
    "        \n",
    "    # show the frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    #cv2.imwrite('images\\\\'+str(ii).zfill(5)+'.png',frame) # Uncomment this to save each frame as .png\n",
    "    ii+=1 # Increment loop count\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    " \n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "        \n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
